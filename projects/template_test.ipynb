{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Generating Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "We begin by generating a test dataset by inserting gaussian noise to a cubic function $f(x) = ax^{3} + bx^{2} + cx + d$. The constants $a$, $b$, $c$ and $d$ will be chosen arbitrarily and the outputted dataset will be saved for future model learning.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 0.7 * (x**3) - 7 * (x**2) + 1 * (x) + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.linspace(-5, 11, 1000, device=device)\n",
    "Y = f(X) + 20 * torch.randn(X.shape[0], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X.cpu(), Y.cpu(), s=2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('cubic function with noise and arbitrary constants')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Creating Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "We now define a linear model that we will train with the generated data in order to *learn* the arbitrarily chosen parameters in the previous step.\n",
    "\n",
    "Our model is a function of $x$, parameterized by weights $a_0,\\, a_1,\\, a_2,\\, a_3$ and will be defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{equation}\n",
    "    f^*(x; a_0,\\, a_1,\\, a_2,\\, a_3) = a_0x^3 + a_1x^2 + a_2x + a_3\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "For coding convenience, we store the constants in a **numpy array:** `a = [a0, a1, a2, a3]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros(4, device=device)\n",
    "\n",
    "def f_star(x):\n",
    "    return a[0]*(x**3) + a[1]*(x**2) + a[2]*x + a[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "The *loss function* which finds the error between our linear model's prediction $f^*(x)$ and the ground truth value $f(x)$ is optimizing the parameters and is thus a function of $\\mathbf{a}$ and parametersized by $x$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{equation}\n",
    "    L(\\mathbf{a};x) = \\frac{1}{2n}\\sum (f^*(x;\\mathbf{a}) - f(x))^2 \\tag{2}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loss(X, Y):\n",
    "    return (1/2) * torch.mean((f_star(X) - Y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "The *gradient* of this loss function $w.r.t$ our unkonwn weights $\\mathbf{a} = [a_0, \\, a_1, \\, a_2, \\, a_3]$ is calculated:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{d}{d\\mathbf{a}} L(\\mathbf{a}; x) &= \\frac{d}{d\\mathbf{a}} \\left[ \\frac{1}{2n}\\sum (f^*(x;\\mathbf{a}) - f(x))^2 \\right] \\\\ \\\\\n",
    "                      &= \\frac{1}{n} \\sum (f^*(x;\\mathbf{a}) - f(x)) \\cdot (\\frac{d}{d\\mathbf{a}} f^*(x;\\mathbf{a})) \\\\ \\\\\n",
    "                      &= \\frac{1}{n} \\sum (f^*(x;\\mathbf{a}) - f(x)) \\cdot \\left[ \\begin{array}{c} x^3 \\\\ x^2 \\\\ x \\\\ 1 \\end{array} \\right]\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, Y):\n",
    "\n",
    "    err = f_star(X) - Y\n",
    "    X_sq = X**2\n",
    "    X_cb = X_sq * X\n",
    "\n",
    "    return torch.tensor([\n",
    "        torch.mean(err * X_cb),\n",
    "        torch.mean(err * X_sq),\n",
    "        torch.mean(err * X),\n",
    "        torch.mean(err * 1)\n",
    "    ], device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "We use this gardient along with a learning rate $\\alpha$ in the **stochastic gradient descent** algorithm to *learn* the true constants (weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomization utility function\n",
    "def randomize_arrays(arrays):\n",
    "    indices = torch.randperm(arrays[0].size(0), device=device)\n",
    "    for arr in arrays:\n",
    "        arr[:] = arr[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = torch.randperm(X.size(0))\n",
    "\n",
    "X_randomized = torch.clone(X)[samples]\n",
    "Y_randomized = torch.clone(Y)[samples]\n",
    "\n",
    "X_train = X_randomized[:int(X_randomized.size(0)*0.95)]\n",
    "Y_train = Y_randomized[:int(Y_randomized.size(0)*0.95)]\n",
    "\n",
    "X_test = X_randomized[int(X_randomized.size(0)*0.95):]\n",
    "Y_test = Y_randomized[int(Y_randomized.size(0)*0.95):]\n",
    "\n",
    "# randomize_arrays([X_train, Y_train])\n",
    "\n",
    "lr = 0.0000005\n",
    "epochs = 30000\n",
    "runs = []\n",
    "\n",
    "batch_size = int(len(X_train)*0.5)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"current weights: {a}\")\n",
    "print(f\"true weights: {[0.7, -7, 1, 5]}\")\n",
    "print()\n",
    "\n",
    "for e in range(epochs):\n",
    "\n",
    "    randomize_arrays([X_train, Y_train])\n",
    "\n",
    "    for batch in range(0, len(X_train), batch_size):\n",
    "        \n",
    "        if (batch + batch_size) > len(X_train):\n",
    "            X_batch = X_train[batch:]\n",
    "            Y_batch = Y_train[batch:]\n",
    "        else:\n",
    "            X_batch = X_train[batch:batch+batch_size]\n",
    "            Y_batch = Y_train[batch:batch+batch_size]\n",
    "\n",
    "        # compute the gradient\n",
    "        grad = gradient(X_batch, Y_batch)\n",
    "\n",
    "        # update the parameters\n",
    "        a -= lr * grad\n",
    "        \n",
    "    if (e % 100 == 0):\n",
    "        # compute the training and testing loss\n",
    "        runs.append([e, a.detach().cpu(), Loss(X_train, Y_train), Loss(X_test, Y_test)])\n",
    "        print(f\"epoch: {e}, gradient: {torch.round(grad)}, updated weights: {torch.round(runs[-1][1])}, train loss: {torch.round(runs[-1][2])}, test loss: {torch.round(runs[-1][3])}\")\n",
    "        print()\n",
    "\n",
    "    if (e % 1000 == 0) and (e != 0):\n",
    "        if((runs[-10][3] - runs[-1][3]) < 0.01):\n",
    "            print(\"convergence, stopping training\")\n",
    "            break\n",
    "        if ((runs[-10][3] - runs[-1][3]) < 1):\n",
    "            lr /= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.animation import PillowWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final results\n",
    "\n",
    "# data\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X_train.cpu(), Y_train.cpu(), 'b.', markersize=2, label='train data')\n",
    "ax.plot(X_test.cpu(), Y_test.cpu(), 'k.', markersize=10, label='test data')\n",
    "\n",
    "# model\n",
    "samples = torch.arange(0, len(X), len(X)//50)\n",
    "X_samples = X[samples]\n",
    "Y_samples = Y[samples]\n",
    "\n",
    "line, = ax.plot([], [], 'r-', label='model')\n",
    "\n",
    "def init():\n",
    "    line.set_data([], [])\n",
    "    return line,\n",
    "\n",
    "def update(frame):\n",
    "    global a\n",
    "    a = runs[frame][1]\n",
    "    line.set_data(X_samples.cpu(), f_star(X_samples).cpu())\n",
    "    return line,\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.set_title('fitting cubic function')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "ani = FuncAnimation(fig, update, frames=range(len(runs)), init_func=init, blit=True, interval=1000//30)\n",
    "\n",
    "plt.close(fig)\n",
    "\n",
    "ani.save(\"modelfit_animation.gif\", writer=PillowWriter(fps=30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "![](modelfit_animation.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
