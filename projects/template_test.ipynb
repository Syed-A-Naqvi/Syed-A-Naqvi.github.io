{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Generating Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "We begin by generating a test dataset by inserting gaussian noise to a cubic function $f(x) = ax^{3} + bx^{2} + cx + d$. The constants $a$, $b$, $c$ and $d$ will be chosen arbitrarily and the outputted dataset will be saved for future model learning.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 0.7 * (x**3) - 7 * (x**2) + 1 * (x) + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(-5, 11, 1000)\n",
    "Y = f(X) + 20 * np.random.randn(X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, Y, s=2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('cubic function with noise and arbitrary constants')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Creating Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "We now define a linear model that we will train with the generated data in order to *learn* the arbitrarily chosen parameters in the previous step.\n",
    "\n",
    "Our model is a function of $x$, parameterized by weights $a_0,\\, a_1,\\, a_2,\\, a_3$ and will be defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{equation}\n",
    "    f^*(x; a_0,\\, a_1,\\, a_2,\\, a_3) = a_0x^3 + a_1x^2 + a_2x + a_3\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "For coding convenience, we store the constants in a **numpy array:** `a = [a0, a1, a2, a3]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros(4)\n",
    "\n",
    "def f_star(x):\n",
    "    return a[0]*(x**3) + a[1]*(x**2) + a[2]*x + a[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "The *loss function* which finds the error between our linear model's prediction $f^*(x)$ and the ground truth value $f(x)$ is optimizing the parameters and is thus a function of $\\mathbf{a}$ and parametersized by $x$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{equation}\n",
    "    L(\\mathbf{a};x) = \\frac{1}{2n}\\sum (f(x) - f^*(x;\\mathbf{a}))^2 \\tag{2}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loss(X, Y):\n",
    "    return (1/2) * np.mean((Y - f_star(X))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "The *gradient* of this loss function $w.r.t$ our unkonwn weights $\\mathbf{a} = [a_0, \\, a_1, \\, a_2, \\, a_3]$ is calculated:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{d}{d\\mathbf{a}} L(\\mathbf{a}; x) &= \\frac{d}{d\\mathbf{a}} \\left[ \\frac{1}{2n}\\sum (f(x) - f^*(x;\\mathbf{a}))^2 \\right] \\\\ \\\\\n",
    "                      &= \\frac{1}{n} \\sum (f(x) - f^*(x;\\mathbf{a})) \\cdot (\\frac{d}{d\\mathbf{a}} f^*(x;\\mathbf{a})) \\\\ \\\\\n",
    "                      &= \\frac{1}{n} \\sum (f(x) - f^*(x;\\mathbf{a})) \\cdot \\left[ \\begin{array}{c} x^3 \\\\ x^2 \\\\ x \\\\ 1 \\end{array} \\right]\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, Y):\n",
    "    \n",
    "    err = Y - f_star(X)\n",
    "\n",
    "    return np.array([\n",
    "        np.mean(err * (X**3)),\n",
    "        np.mean(err * (X**2)),\n",
    "        np.mean(err * X),\n",
    "        np.mean(err * 1)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "We use this gardient along with a learning rate $\\alpha$ in the **stochastic gradient descent** algorithm to *learn* the true constants (weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:int(len(X)*0.95)]\n",
    "Y_train = Y[:int(len(Y)*0.95)]\n",
    "X_test = X[int(len(X)*0.95):]\n",
    "Y_test = Y[int(len(Y)*0.95):]\n",
    "\n",
    "lr = 0.000000001\n",
    "epochs = 5000\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "for e in range(epochs):\n",
    "\n",
    "    # compute the gradient\n",
    "    grad = gradient(X_train, Y_train)\n",
    "    \n",
    "    # update the parameters\n",
    "    a += lr * (-grad)\n",
    "\n",
    "    \n",
    "    if (e % 10 == 0):\n",
    "        # compute the training and testing loss\n",
    "        train_loss.append([e, Loss(X_train, Y_train)])\n",
    "        test_loss.append([e, Loss(X_test, Y_test)])\n",
    "\n",
    "    if (e % 10 == 0) or (e == epochs - 1):\n",
    "        print(f\"epoch: {e}, train loss: {train_loss[e//10][1]}, test loss: {test_loss[e//10][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, Y, 'b.', markersize=2, label='data')\n",
    "plt.plot(X, f_star(X), 'r-', label='model')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('fitted cubic function')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = np.array(test_loss)\n",
    "train_loss = np.array(train_loss)\n",
    "print(min(test_loss[:,1]), min(train_loss[:,1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
